\newpage
\vspace*{\fill}
\begin{center}
    \section*{\huge Abstract}
    \addcontentsline{toc}{chapter}{Abstract}
\end{center}
The word `GPT-3' has brought the tech world to an unspoken frenzy. It seems to keep on appearing over and over again, all across the technical media corpus. GPT-3 stands for Generative Pre-trained Transformer, which is a language model, created by the engineers at OpenAI. A language model takes in sequences of text input and spits out another sequence of coherent text, which is in some way related to the input. Parameters in a machine learning models can be thought of as knobs and dials of a function which are continuously tweaked until an optimal desired response is obtained from the model. This model in massive, in the sense that it has huge number of these parameters. Even more interesting is its incredible capability to generate coherent literature. The media is hyped with the mind boggling, selected results of GPT-3. The report aims to delves deep into GPT-3 to realize its architecture and how it works. To be unbiased, this report also explores what some of the critiques has voiced and includes multiple perspectives. Finally it concludes with certain interesting remarks on GPT-3.
\vspace*{\fill}